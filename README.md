# Wyoming Kokoro Torch

[Wyoming protocol](https://github.com/rhasspy/wyoming) server for the original [Kokoro](https://github.com/hexgrad/kokoro/) Torch TTS implementation.

Contrary to other Wyoming implementation, [wyoming-kokoro](https://github.com/nordwestt/kokoro-wyoming/), this is one uses Torch instead of ONNX.
As of the time of writing, our implementation also supports `streaming` mode, while the ONNX one doesn't. Streaming is important for LLM-based assistant,
so that it can start speaking before the LLM is finished generating.

Note: the audio generated by kokoro is a bit quiet out of the box. It's recommended to set the volume to 2.0 by passing `--volume 2.0` to the application at startup.

## Local Install
Now support automatic GPU detection using CUDA (NVIDIA) and MPS (Apple Silicon). The required PyTorch version must be installed separately using the new --device argument during setup.

### 1. Clone and Setup
Clone the repository and set up the Python virtual environment:

``` sh
git clone https://github.com/debackerl/wyoming-kokoro-torch.git
cd wyoming-kokoro-torch

# Option 1: CPU (Default)
# Use this if you do not have a supported GPU.
script/setup --device cpu

# Option 2: CUDA (NVIDIA GPUs)
# Use this if you have an NVIDIA GPU and CUDA installed.
script/setup --device cuda

# Option 3: MPS (Apple Silicon M-series Macs)
# Use this if you are running on an Apple M1/M2/M3 chip.
script/setup --device mps
```

### 2. Download Base Model
Download the base model:

```sh
mkdir /data
wget -O /data/kokoro-v1_0.pth https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/kokoro-v1_0.pth
wget -O /data/config.json https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/config.json
```

### 3. Run Server
The server will automatically use the fastest available device (CUDA $\rightarrow$ MPS $\rightarrow$ CPU) unless the --device flag is explicitly set during runtime.

```sh
# 1. Automatic Detection (Example result: CUDA if installed, otherwise MPS, then CPU)
script/run --voice af_heart --streaming --uri 'tcp://0.0.0.0:10300' --data-dir /data --download-dir /data

# 2. Force CPU (Explicitly tells PyTorch to use the CPU backend)
script/run --voice af_heart --streaming --uri 'tcp://0.0.0.0:10300' --data-dir /data --download-dir /data --device cpu

# 3. Force CUDA (Explicitly tells PyTorch to use the CUDA backend)
script/run --voice af_heart --streaming --uri 'tcp://0.0.0.0:10300' --data-dir /data --download-dir /data --device cuda
```

See [available voices](https://huggingface.co/hexgrad/Kokoro-82M/tree/main/voices).

## Remarks

If you run this in a VM, you may see `Could not initialize NNPACK! Reason: Unsupported hardware.` in the logs. This seems to happen on heterogeneous CPU architectures,
like my AMD Ryzen AI 7 HX 370. Solutions seems to be either running bare-metal or pass the L3 cache information to the VM and tweak CPU affinity.
